{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic LLM\n",
    "\n",
    "[李沐讲论文](https://www.youtube.com/watch?v=iqX0pgNDon0&list=RDCMUC8WCW6C3BWLKSZ5cMzD8Gyw&index=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 这个工作和之前InstructGTP的工作类似，只不过用了两个数据集，一个是无害性数据集，一个是有用性数据集，训练一个PM模型，对机器的response打分。训练好这个PM模型作为后面强化学习的奖励函数\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PM模型预训练一个语言模型，类似GTP\n",
    "2. 从网上StackExchange等网站找一些对话作为训练样本，这些对话的response本身网站上就有用户的打分。这部分数据比较多，用的学习率比较高\n",
    "3. 前面收集的人类打分的数据，这部分数据少，学习率较低，作微调，防止过拟合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](03.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 强化学习过程，用前面的PM模型作奖励函数，同时加入了一项KL散度惩罚项，防止过拟合（经典的做法），使得每次模型较之前变化不至于过大"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
