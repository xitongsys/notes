{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT1-3\n",
    "\n",
    "[李沐讲论文](https://www.youtube.com/watch?v=t70Bl3w7bxY&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT1: \n",
    "![](01.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 之所以用decoder，是指的利用其中的mask，把当前以及后面的词屏蔽掉"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](03.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. L1 就是前面pre-trainning的L。也就是在tuning的时候，同时也训练前面训练过得transformer模型的参数。同时把这两个目标函数做个结合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](04.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Extract`,`Delim`,`Start`都是特殊的token，用于最后的分类等任务"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "![](05.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](06.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. McCann 就是想用一个模型来完成多个任务。所以在训练模型的时候，就把要做的任务也作为input输入到模型中，构造新的序列去训练。像文中的`translate to french`,`answer the question`。这种也就是所谓的prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](07.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GPT2则更进了一步，认为文本中已经包含了这些prompt的信息。只要模型足够大，就能够学习到这种关系。比如那个例子。文本中有些英语翻译法语的例子，模型通过读取这些信息，就能够学会英语翻译法语。而这些句子中间的其他词，也就是prompt，无需人为构造"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT3\n",
    "![](08.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](09.png)\n",
    "![](10.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 传统的fine-tuning，就是用有标记的数据，去训练pre-train的模型，同时跟新其参数\n",
    "\n",
    "2. GPT则采用了上下文学习，不去更新其权重（模型太大了）。这种基于上下文的学习(in-context learning)，就是采用提供一个指令，以及一些example，直接让模型输出结果。而这种能力，正是基于transformer和大量文本学习得到的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](11.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在处理Common Crawl脏数据的，时候，其中，1是说的用reddit数据作为正数据集，common crawl的作为负数据集。然后训练一个Logistic Regression二分类器。然后用这个分类器去预测所有common crawl的结果，如果结果偏正，就纳入训练集。NICE！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
