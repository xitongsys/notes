{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "[李沐讲论文]()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.png)\n",
    "\n",
    "1. 在计算下一个输出的时候，会把之前的输出也作为输入，这个就叫auto-regressive。也就是图中右半部分最下面output(shifted right)。比如输入英文，输出中文，那么会把前面输出的中文也作为解码器的输入\n",
    "\n",
    "2. 就是因为有自回归，所以解码器在计算attention的时候，增加了mask，防止看到后面的内容"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](02.png)\n",
    "\n",
    "1. Batch Norm 和 Layer Norm\n",
    "\n",
    "以二维为例，每一行是一个样本，每个样本有d个feature。Batch Norm就是按照batch的维度去做normalization，也就是把不同batch里的相同feature位置的值作归一。也就是下图中的蓝线部分。而layer norm 是把一个样本中的不同feature的值去做归一，也就是下图中的黄线部分。\n",
    "\n",
    "对于3维情况也是类似的（图中立方体），比如机器翻译，每次输入的是一个序列，每个位置是一个词向量。词向量的每个位置是一个feature。如果作batch norm，就是把相同feature位置所有batch的值去做归一（蓝线）。而layer norm是把同一个batch里面所有不同feature的值作归一（黄线）\n",
    "\n",
    "![](03.png)\n",
    "\n",
    "\n",
    "之所以用layer norm比较多，是因为输入的序列往往长度不一样。也就是不同的batch长度不一样。如果按照batch norm，那么空的地方要填入0，对计算均值和方差是有影响的。而layer norm不存在这个问题\n",
    "\n",
    "![](04.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](05.png)\n",
    "\n",
    "1. 注意力机制就是计算某个向量$(query)$和其他一些已知向量$[(key_i,value_i)]$的相似度，然后作加权和。计算各个相似度用内积 $query \\cdot key_i$，然后用这个相似度作权重（计算权重用softmax)，乘以$value_i$，再求和。\n",
    "\n",
    "$ attention = \\sum_{i=0}^n softmax((query \\cdot key_i))  value_i $\n",
    "\n",
    "2. 当有多个query的时候，可以用矩阵运算，也就是图中的公式\n",
    "\n",
    "![](06.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
