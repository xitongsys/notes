{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n",
    "\n",
    "[李沐讲论文](https://www.youtube.com/watch?v=luP3-Fs0QCo&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Swin Transformer 解决了两个问题，一个是传统ViT的feature维度是固定的，不能像CNN那样拿到图片的多尺度信息。另一个之前的ViT是计算全局的transformer，导致计算复杂度较大。而Swin则只计算紧邻block的，降低了复杂度\n",
    "\n",
    "2. 因为有shift和多层transformer，是的swin transformer也可以拿到不同block之间的关系。相当于计算了全局的transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](03.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. patch merging，类似于CNN中池化操作，本质上是一个下采样\n",
    "\n",
    "![](04.png)\n",
    "\n",
    "比如把4x4的一个大batch，采样成2x2的4个小patch，可以如图中所示间隔采样，最后合并成一个"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](06.png)\n",
    "![](05.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. shift的过程，如上图。shift之后，每个窗口大小不一样。计算不方便。一种方法是直接填充，但是影响性能。所以文中其实做了一个周期边边条件扩展。使得每个窗口大小一样了。但是带来的问题是，本来不相邻，相隔很远的像素，现在相邻了，也要计算transformer。所以为了避免这种情况，他们提出了掩码计算的方法。本质上就是把不该计算但是已经计算的位置屏蔽掉\n",
    "\n",
    "[ref: github issue](https://github.com/microsoft/Swin-Transformer/issues/38)\n",
    "\n",
    "![](07.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
