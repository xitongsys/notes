{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "[李沐讲论文](https://www.youtube.com/watch?v=NnSldWhSqvY&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.png)\n",
    "![](02.png)\n",
    "\n",
    "1. 深度神经网络的两个问题，一个是梯度消失/爆炸（vanishing/exploding gradients）。这个可应通过初始化和中间加入normalization解决。另一个问题就是随着层数增加，精度下降。无论是训练误差还是测试误差，都会上升\n",
    "\n",
    "2. 残差网络其实就是让新加的layer去学习$targe - input$的残差"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](03.png)\n",
    "\n",
    "1. $1\\times1$ kernel\n",
    "   \n",
    "如果输入的depth 是 3，那么$1\\times1$的kernel的维度是$1x1x3$，这三个数就是上一层输入的3个channel的不同权重。其实本质和convolution是一样的。巻积是在平面上加权降维。而这个操作是在深度上加权降维\n",
    "\n",
    "\n",
    "![](05.png)\n",
    "![](06.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](04.png)\n",
    "\n",
    "1. error 突然下降的地方是调整学习率造成的"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
