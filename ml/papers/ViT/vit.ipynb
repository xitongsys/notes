{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT\n",
    "\n",
    "[李沐讲论文](https://www.youtube.com/watch?v=FRFt3x0bO94&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.png)\n",
    "\n",
    "1. Inductive biases: 一些先验的知识。比如CNN中，由于巻积核特有的性质，使其天然就可以拿到图片的局部特征。同时巻积核的平移过程，同一个像素点如果在原始空间发生了平移，在巻积之后的空间也发生了同样的平移。这也就拿到了空间特征。而这些在Transformer里面是没有的\n",
    "\n",
    "2. [translation equivariant](https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59)\n",
    "\n",
    "![](02.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](03.png)\n",
    "\n",
    "1. BERT 采用的方法类似完形填空。把初始句子挖掉一些地方，让模型去预测删除的词。GPT是根据前面的词去预测句子后面的词\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](04.png)\n",
    "\n",
    "1. 第一个token就是类别向量\n",
    "\n",
    "2. ViT 中只用了transformer的encoder，图中输入了10个token，输出也会是10个token。最终会有这个位置的输出作为分类用的token\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](05.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
