{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "\n",
    "![](bellman01.png)\n",
    "![](bellman02.png)\n",
    "![](bellman03.png)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Q-table to solve Maze \n",
    "\n",
    "[reference](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforcement learning: Maze\n",
    "\n",
    "MAZE = [\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "MAZE = [\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "R, C = len(MAZE), len(MAZE[0])\n",
    "\n",
    "# Define the rewards for every step\n",
    "ACTIONS = [[1,0], [-1,0], [0,-1], [0,1]]\n",
    "ACTIONS_STR = ['v', '^', '<', '>']\n",
    "REWARDS = np.zeros((R, C))\n",
    "for i in range(R):\n",
    "    for j in range(C):\n",
    "        if MAZE[i][j] == 0:\n",
    "            r = -0.1\n",
    "        elif MAZE[i][j] == 1:\n",
    "            r = -1\n",
    "        REWARDS[i][j] = r\n",
    "REWARDS[R-1][C-1] = 1\n",
    "\n",
    "def go_one_step(state:int, action:int):\n",
    "    i, j = state//C, state%C\n",
    "    ii, jj = i + ACTIONS[action][0], j + ACTIONS[action][1]\n",
    "    new_state = ii * C + jj\n",
    "\n",
    "    done = False\n",
    "    if ii == R-1 and jj == C-1:\n",
    "        done = True\n",
    "        \n",
    "    if ii < 0 or ii >= R or jj < 0 or jj >= C:\n",
    "        r = -1\n",
    "        new_state = state\n",
    "    else:\n",
    "        r = REWARDS[ii][jj]\n",
    "    \n",
    "    return new_state, r, done\n",
    "\n",
    "def avail_actions(state:int):\n",
    "    res = []\n",
    "    i, j = state//C, state%C\n",
    "    for a in range(len(ACTIONS)):\n",
    "        ii, jj = i + ACTIONS[a][0], j + ACTIONS[a][1]\n",
    "        if ii >= 0 and ii < R and jj >= 0 and jj < C:\n",
    "            res.append(a)\n",
    "    return res\n",
    "            \n",
    "        \n",
    "        \n",
    "action_size = 4 # ^ v < >\n",
    "state_size = R * C\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "for state in range(state_size):\n",
    "    acts = avail_actions(state)\n",
    "    for a in range(4):\n",
    "        if a not in acts:\n",
    "            qtable[state, a] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_episodes = 1500 # Total episodes\n",
    "learning_rate = 0.8 # Learning rate\n",
    "max_step = 99 # Max step per episode\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Q-table\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = 0\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        rnd = random.uniform(0,1)\n",
    "        if rnd > epsilon:\n",
    "            action = np.argmax(qtable[state, :])\n",
    "        else:\n",
    "            action = random.sample(avail_actions(state),1)[0]\n",
    "            \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        print(state, new_state)\n",
    "        \n",
    "        ## Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "print(\"Score over time: \" + str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Maze\n",
    "\n",
    "for episode in range(1):\n",
    "    state = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        action = np.argmax(qtable[state, :])\n",
    "        \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        state = new_state\n",
    "        actions.append(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(list(map(lambda a: ACTIONS_STR[a], actions)))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Deep Q Network\n",
    "\n",
    "[REF 1](https://github.com/mswang12/minDQN/blob/main/minDQN.ipynb)\n",
    "[REF 2](https://zhuanlan.zhihu.com/p/110769361)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的DQN\n",
    "\n",
    "只需要用DNN代替前面的Q table即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net: QTable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "\n",
    "dqn_target = Net(n_feature=1, n_hidden=10, n_output=4)    \n",
    "dqn = Net(n_feature=1, n_hidden=10, n_output=4)\n",
    "\n",
    "def trainDQN(x, y):\n",
    "    optimizer = torch.optim.Adam(dqn.parameters(), lr=0.01)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    for t in range(100):\n",
    "        py = dqn(x)\n",
    "        loss = loss_func(py, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre train        \n",
    "for i in range(10):\n",
    "    n = 100000\n",
    "    x = torch.from_numpy(np.array([[random.randint(0, R*C)] for _ in range(n)])).type(torch.FloatTensor)\n",
    "    y = torch.from_numpy(np.array([[1,1,1,1] for _ in range(n)])).type(torch.FloatTensor)\n",
    "    trainDQN(x, y)\n",
    "        \n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "\n",
    "for state in range(R*C):\n",
    "    x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "    y = dqn(x).data.numpy()[0]\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_episodes = 500 # Total episodes\n",
    "learning_rate = 0.8 # Learning rate\n",
    "max_step = 99 # Max step per episode\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = 0\n",
    "\n",
    "    for step in range(max_step):\n",
    "    #while True:\n",
    "        rnd = random.uniform(0,1)\n",
    "        # \n",
    "        x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "    \n",
    "        qs = dqn(x).data.numpy()[0]\n",
    "        \n",
    "        if rnd > epsilon:\n",
    "            action, mx = 0, -100000\n",
    "            for a in avail_actions(state):\n",
    "                if qs[a] >= mx:\n",
    "                    action = a\n",
    "                    mx = qs[a]\n",
    "            \n",
    "        else:\n",
    "            action = random.sample(avail_actions(state),1)[0]\n",
    "            \n",
    "        new_state, reward, done = go_one_step(state, action)        \n",
    "        new_x = torch.from_numpy(np.array([[new_state]])).type(torch.FloatTensor)\n",
    "        new_qs = dqn_target(new_x).data.numpy()[0]\n",
    "        \n",
    "        #print(qs,state, new_state,  action, reward, done)\n",
    "        \n",
    "        print(state, new_state)\n",
    "        \n",
    "        if done:\n",
    "            max_future_q = reward\n",
    "        else:\n",
    "            max_future_q = reward + gamma*np.max(new_qs)\n",
    "        \n",
    "        \n",
    "        ## Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        #qs[action] = qs[action] + learning_rate*(reward+gamma*np.max(new_qs) - qs[action])\n",
    "        \n",
    "        qs[action] = (1-learning_rate)*qs[action] + learning_rate*max_future_q\n",
    "        \n",
    "        #print(qs)\n",
    "        \n",
    "        if step % 4 == 0:\n",
    "            # update DQN\n",
    "            x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "            y = torch.from_numpy(np.array([qs])).type(torch.FloatTensor)\n",
    "            for i in range(100):\n",
    "                trainDQN(x, y)    \n",
    "            \n",
    "        \n",
    "        qs = dqn(x).data.numpy()[0]\n",
    "        #print(qs)\n",
    "    \n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "    print(total_rewards)\n",
    "    \n",
    "    if episode % 1 == 0:\n",
    "        dqn_target.load_state_dict(dqn.state_dict())\n",
    "    \n",
    "print(\"Score over time: \" + str(sum(rewards)/total_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Maze\n",
    "\n",
    "for episode in range(1):\n",
    "    state = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "        qs = dqn_target(x).data.numpy()[0]\n",
    "        action, mx = 0, -100000\n",
    "        for a in avail_actions(state):\n",
    "            if qs[a] >= mx:\n",
    "                action = a\n",
    "                mx = qs[a]\n",
    "        \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        print(state, action, reward, qs)\n",
    "        \n",
    "        state = new_state\n",
    "        actions.append(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(list(map(lambda a: ACTIONS_STR[a], actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in range(R*C):\n",
    "    x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "    y = dqn(x).data.numpy()[0]\n",
    "    print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 三个优化\n",
    "\n",
    "![](dqn01.png)\n",
    "![](dqn02.png)\n",
    "![](dqn03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
