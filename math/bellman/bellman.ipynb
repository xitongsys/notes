{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "\n",
    "![](bellman01.png)\n",
    "![](bellman02.png)\n",
    "![](bellman03.png)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Q-table to solve Maze \n",
    "\n",
    "[reference](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforcement learning: Maze\n",
    "\n",
    "MAZE = [\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "MAZE = [\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 0],\n",
    "]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "R, C = len(MAZE), len(MAZE[0])\n",
    "\n",
    "# Define the rewards for every step\n",
    "ACTIONS = [[1,0], [-1,0], [0,-1], [0,1]]\n",
    "ACTIONS_STR = ['v', '^', '<', '>']\n",
    "REWARDS = np.zeros((R, C))\n",
    "for i in range(R):\n",
    "    for j in range(C):\n",
    "        if MAZE[i][j] == 0:\n",
    "            r = -0.1\n",
    "        elif MAZE[i][j] == 1:\n",
    "            r = -1\n",
    "        REWARDS[i][j] = r\n",
    "REWARDS[R-1][C-1] = 100\n",
    "\n",
    "def go_one_step(state:int, action:int):\n",
    "    i, j = state//C, state%C\n",
    "    ii, jj = i + ACTIONS[action][0], j + ACTIONS[action][1]\n",
    "    new_state = ii * C + jj\n",
    "\n",
    "    done = False\n",
    "    if ii == R-1 and jj == C-1:\n",
    "        done = True\n",
    "        \n",
    "    if ii < 0 or ii >= R or jj < 0 or jj >= C:\n",
    "        r = -1\n",
    "        new_state = state\n",
    "    else:\n",
    "        r = REWARDS[ii][jj]\n",
    "    \n",
    "    return new_state, r, done\n",
    "\n",
    "def avail_actions(state:int):\n",
    "    res = []\n",
    "    i, j = state//C, state%C\n",
    "    for a in range(len(ACTIONS)):\n",
    "        ii, jj = i + ACTIONS[a][0], j + ACTIONS[a][1]\n",
    "        if ii >= 0 and ii < R and jj >= 0 and jj < C:\n",
    "            res.append(a)\n",
    "    return res\n",
    "            \n",
    "        \n",
    "        \n",
    "action_size = 4 # ^ v < >\n",
    "state_size = R * C\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "for state in range(state_size):\n",
    "    acts = avail_actions(state)\n",
    "    for a in range(4):\n",
    "        if a not in acts:\n",
    "            qtable[state, a] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_episodes = 10000 # Total episodes\n",
    "learning_rate = 0.8 # Learning rate\n",
    "max_step = 99 # Max step per episode\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 99.48714999998464\n",
      "[[ 80.1796375   -1.          -1.          81.0796375 ]\n",
      " [ 85.45225     -1.          76.92565563  83.74      ]\n",
      " [ 89.2         -1.          81.0796375   -1.        ]\n",
      " [ 84.55225     76.92565563  -1.          85.45225   ]\n",
      " [ 90.055       81.0796375   80.1796375   89.2       ]\n",
      " [ 94.          83.74        85.45225     -1.        ]\n",
      " [ 90.055       80.1796375   -1.          90.055     ]\n",
      " [ 94.9         85.45225     84.55225     94.        ]\n",
      " [100.          89.2         90.055       -1.        ]\n",
      " [ -1.          84.55225     -1.          94.9       ]\n",
      " [ -1.          90.055       90.055      100.        ]\n",
      " [ -1.           0.           0.          -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Train the Q-table\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = 0\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        rnd = random.uniform(0,1)\n",
    "        if rnd > epsilon:\n",
    "            action = np.argmax(qtable[state, :])\n",
    "        else:\n",
    "            action = random.sample(avail_actions(state),1)[0]\n",
    "            \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        #print(state, new_state)\n",
    "        \n",
    "        ## Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "print(\"Score over time: \" + str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', 'v', 'v', 'v', '>']\n"
     ]
    }
   ],
   "source": [
    "# Solve the Maze\n",
    "\n",
    "for episode in range(1):\n",
    "    state = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        action = np.argmax(qtable[state, :])\n",
    "        \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        state = new_state\n",
    "        actions.append(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(list(map(lambda a: ACTIONS_STR[a], actions)))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Deep Q Network\n",
    "\n",
    "[REF 1](https://github.com/mswang12/minDQN/blob/main/minDQN.ipynb)\n",
    "[REF 2](https://zhuanlan.zhihu.com/p/110769361)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "1. 只需要用DNN代替前面的Q table即可\n",
    "2. 经测试发现，replay batch train特别重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_episodes = 100 # Total episodes\n",
    "learning_rate = 0.8 # Learning rate\n",
    "max_step = 99 # Max step per episode\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net: QTable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "\n",
    "dqn_target = Net(n_feature=1, n_hidden=100, n_output=4)    \n",
    "dqn = Net(n_feature=1, n_hidden=100, n_output=4)\n",
    "\n",
    "replay_memory = []\n",
    "\n",
    "def trainDQN(x, y):\n",
    "    optimizer = torch.optim.Adam(dqn.parameters(), lr=0.01)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    for t in range(500):\n",
    "        py = dqn(x)\n",
    "        loss = loss_func(py, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def trainDQNBatch():\n",
    "    global replay_memory\n",
    "    if len(replay_memory) > 100000:\n",
    "        replay_memory = replay_memory[:-100000]\n",
    "    \n",
    "    batch_size = min(64*2, len(replay_memory))\n",
    "    mini_batch = random.sample(replay_memory, batch_size)  \n",
    "    \n",
    "    X, Y = [], []\n",
    "    for state, action, reward, new_state, done in mini_batch:\n",
    "        x = torch.from_numpy(np.array([state])).type(torch.FloatTensor)\n",
    "        new_x = torch.from_numpy(np.array([new_state])).type(torch.FloatTensor)\n",
    "        qs = dqn(x).data.numpy()\n",
    "        new_qs = dqn_target(new_x).data.numpy()\n",
    "        if not done:\n",
    "            max_future_q = reward + gamma * np.max(new_qs)\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "        \n",
    "        #print(action, qs)\n",
    "        qs[action] = (1-learning_rate)*qs[action] + learning_rate*max_future_q\n",
    "        \n",
    "        X.append([state])\n",
    "        Y.append(qs)\n",
    "    \n",
    "    X = torch.from_numpy(np.array(X)).type(torch.FloatTensor)\n",
    "    Y = torch.from_numpy(np.array(Y)).type(torch.FloatTensor)\n",
    "    \n",
    "    trainDQN(X, Y)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6260892e-06  4.9564987e-06  1.1920929e-07  6.0424209e-06]\n",
      "[-4.52343374e-05 -1.04030594e-04  7.54594803e-05 -3.98976728e-04]\n",
      "[ 0.00075101  0.00075611 -0.00104138  0.00476136]\n",
      "[-0.00040017 -0.00029201  0.00043276 -0.00186353]\n",
      "[-0.00030367 -0.00023956  0.00035611 -0.00154104]\n",
      "[-0.00021518 -0.00018693  0.0002754  -0.00121599]\n",
      "[-0.00012655 -0.00013346  0.00019464 -0.00089099]\n",
      "[-3.806688e-05 -8.059107e-05  1.141727e-04 -5.663242e-04]\n",
      "[ 5.0654635e-05 -2.7602538e-05  3.3229589e-05 -2.4106167e-04]\n",
      "[ 1.3906322e-04  2.5326386e-05 -4.7355890e-05  8.3724037e-05]\n",
      "[ 2.2778474e-04  7.7897683e-05 -1.2782216e-04  4.0868856e-04]\n",
      "[ 0.00031625  0.0001313  -0.00020888  0.00073359]\n"
     ]
    }
   ],
   "source": [
    "# pre train        \n",
    "for i in range(1):\n",
    "    n = 1000\n",
    "    x = torch.from_numpy(np.array([[random.randint(0, R*C)] for _ in range(n)])).type(torch.FloatTensor)\n",
    "    y = torch.from_numpy(np.array([[0,0,0,0] for _ in range(n)])).type(torch.FloatTensor)\n",
    "    trainDQN(x, y)\n",
    "        \n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "\n",
    "for state in range(R*C):\n",
    "    x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "    y = dqn(x).data.numpy()[0]\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = 0\n",
    "    step = 0\n",
    "\n",
    "    #for step in range(max_step):\n",
    "    while True:\n",
    "        step += 1\n",
    "        rnd = random.uniform(0,1)\n",
    "        x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "    \n",
    "        qs = dqn(x).data.numpy()[0]\n",
    "        \n",
    "        if rnd > epsilon:\n",
    "            action, mx = 0, -100000\n",
    "            for a in avail_actions(state):\n",
    "                if qs[a] >= mx:\n",
    "                    action = a\n",
    "                    mx = qs[a]\n",
    "            \n",
    "        else:\n",
    "            action = random.sample(avail_actions(state),1)[0]\n",
    "            \n",
    "        new_state, reward, done = go_one_step(state, action)               \n",
    "        \n",
    "        replay_memory.append([state, action, reward, new_state, done])\n",
    "        \n",
    "        if step % 4 == 0:\n",
    "            trainDQNBatch()    \n",
    "        \n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "    print(total_rewards)\n",
    "    \n",
    "    if episode % 1 == 0:\n",
    "        dqn_target.load_state_dict(dqn.state_dict())\n",
    "    \n",
    "print(\"Score over time: \" + str(sum(rewards)/total_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 3 -0.1 [ 80.17061  -17.889841  71.95214   81.083496]\n",
      "1 4 0 -0.1 [85.46958  13.561689 76.93455  83.75208 ]\n",
      "4 7 0 -0.1 [90.06905  81.09974  80.200005 89.2372  ]\n",
      "7 10 0 -0.1 [94.94379 85.48391 84.69894 93.9384 ]\n",
      "10 11 3 100.0 [ 6.709077 90.10781  90.137184 99.97809 ]\n",
      "['>', 'v', 'v', 'v', '>']\n"
     ]
    }
   ],
   "source": [
    "# Solve the Maze\n",
    "\n",
    "for episode in range(1):\n",
    "    state = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "        qs = dqn_target(x).data.numpy()[0]\n",
    "        action, mx = 0, -100000\n",
    "        for a in avail_actions(state):\n",
    "            if qs[a] >= mx:\n",
    "                action = a\n",
    "                mx = qs[a]\n",
    "        \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        print(state, new_state, action, reward, qs)\n",
    "        \n",
    "        state = new_state\n",
    "        actions.append(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(list(map(lambda a: ACTIONS_STR[a], actions)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 三个优化\n",
    "\n",
    "![](dqn01.png)\n",
    "![](dqn02.png)\n",
    "![](dqn03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
