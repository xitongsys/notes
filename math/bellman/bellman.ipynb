{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "\n",
    "![](bellman01.png)\n",
    "![](bellman02.png)\n",
    "![](bellman03.png)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Q-table to solve Maze \n",
    "\n",
    "[reference](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforcement learning: Maze\n",
    "\n",
    "MAZE = [\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "R, C = len(MAZE), len(MAZE[0])\n",
    "\n",
    "# Define the rewards for every step\n",
    "ACTIONS = [[1,0], [-1,0], [0,-1], [0,1]]\n",
    "ACTIONS_STR = ['v', '^', '<', '>']\n",
    "REWARDS = np.zeros((R, C))\n",
    "for i in range(R):\n",
    "    for j in range(C):\n",
    "        if MAZE[i][j] == 0:\n",
    "            r = -1\n",
    "        elif MAZE[i][j] == 1:\n",
    "            r = -100\n",
    "        REWARDS[i][j] = r\n",
    "REWARDS[R-1][C-1] = 10000\n",
    "\n",
    "def go_one_step(state:int, action:int):\n",
    "    i, j = state//C, state%C\n",
    "    ii, jj = i + ACTIONS[action][0], j + ACTIONS[action][1]\n",
    "    new_state = ii * C + jj\n",
    "\n",
    "    done = False\n",
    "    if ii == R-1 and jj == C-1:\n",
    "        done = True\n",
    "        \n",
    "    if ii < 0 or ii >= R or jj < 0 or jj >= C:\n",
    "        r = -100000\n",
    "        new_state = state\n",
    "    else:\n",
    "        r = REWARDS[ii][jj]\n",
    "    \n",
    "    return new_state, r, done\n",
    "\n",
    "def avail_actions(state:int):\n",
    "    res = []\n",
    "    i, j = state//C, state%C\n",
    "    for a in range(len(ACTIONS)):\n",
    "        ii, jj = i + ACTIONS[a][0], j + ACTIONS[a][1]\n",
    "        if ii >= 0 and ii < R and jj >= 0 and jj < C:\n",
    "            res.append(a)\n",
    "    return res\n",
    "            \n",
    "        \n",
    "        \n",
    "action_size = 4 # ^ v < >\n",
    "state_size = R * C\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "for state in range(state_size):\n",
    "    acts = avail_actions(state)\n",
    "    for a in range(4):\n",
    "        if a not in acts:\n",
    "            qtable[state, a] = -10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_episodes = 15000 # Total episodes\n",
    "learning_rate = 0.8 # Learning rate\n",
    "max_step = 99 # Max step per episode\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 9961.543266666667\n",
      "[[  6196.09908544 -10000.         -10000.           6295.09908544]\n",
      " [  6627.47272152 -10000.           5979.34413117   6528.47272152]\n",
      " [  6977.33970686 -10000.           6295.09908544   6977.33970686]\n",
      " [  7345.62074406 -10000.           6528.47272152   7269.01638346]\n",
      " [  7553.64413106 -10000.           6977.33970659   7652.648875  ]\n",
      " [  8056.4725     -10000.           7269.01627926 -10000.        ]\n",
      " [  6528.47271727   5979.34413117 -10000.           6627.47272152]\n",
      " [  6784.28970686   6295.09908544   6196.09908544   6977.33970686]\n",
      " [  7246.62074406   6528.47272152   6627.47272152   7345.62074406]\n",
      " [  7733.28499375   6977.33970686   6977.33970686   7553.648875  ]\n",
      " [  8042.352625     7269.01643094   7345.62074406   8056.4725    ]\n",
      " [  8481.55         7652.64887428   7553.648875   -10000.        ]\n",
      " [  6977.33970683   6196.08148003 -10000.           6784.28970686]\n",
      " [  7246.62074406   6627.47272147   6528.47271302   7246.62074406]\n",
      " [  7733.28499375   6977.33970686   6784.28970686   7733.28499375]\n",
      " [  8141.352625     7345.62074406   7246.62074406   8042.352625  ]\n",
      " [  8570.8975       7553.64887483   7733.28499375   8481.55      ]\n",
      " [  8929.           8056.4725       8042.352625   -10000.        ]\n",
      " [  7345.62074406   6528.47267369 -10000.           7246.62074264]\n",
      " [  7733.28499312   6784.28970398   6977.32599047   7733.28499375]\n",
      " [  8042.352625     7246.62074406   7246.62074406   8141.352625  ]\n",
      " [  8471.8975       7733.28499375   7733.28499375   8570.8975    ]\n",
      " [  9023.05         8042.352625     8141.352625     8929.        ]\n",
      " [  9400.           8481.55         8570.8975     -10000.        ]\n",
      " [  7733.27413413   6977.33874748 -10000.           7733.28499375]\n",
      " [  8141.352625     7246.62074406   7345.62074105   8042.3526249 ]\n",
      " [  8570.8975       7733.28499375   7733.28499375   8471.8975    ]\n",
      " [  9023.05         8141.352625     8042.352625     9023.05      ]\n",
      " [  9499.           8570.8975       8471.8975       9400.        ]\n",
      " [ 10000.           8929.           9023.05       -10000.        ]\n",
      " [-10000.           7345.61374635 -10000.           8141.35262494]\n",
      " [-10000.           7733.28499375   7733.28499187   8570.8975    ]\n",
      " [-10000.           8042.352625     8141.352625     9023.05      ]\n",
      " [-10000.           8471.8975       8570.8975       9499.        ]\n",
      " [-10000.           9023.05         9023.05        10000.        ]\n",
      " [-10000.              0.              0.         -10000.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Train the Q-table\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = 0\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        rnd = random.uniform(0,1)\n",
    "        if rnd > epsilon:\n",
    "            action = np.argmax(qtable[state, :])\n",
    "        else:\n",
    "            action = random.sample(avail_actions(state),1)[0]\n",
    "            \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        ## Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "print(\"Score over time: \" + str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', 'v', '>', '>', 'v', 'v', '>', 'v', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Solve the Maze\n",
    "\n",
    "for episode in range(1):\n",
    "    state = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        action = np.argmax(qtable[state, :])\n",
    "        \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        actions.append(action)\n",
    "        \n",
    "    print(list(map(lambda a: ACTIONS_STR[a], actions)))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Deep Q Network\n",
    "\n",
    "[REF 1](https://github.com/mswang12/minDQN/blob/main/minDQN.ipynb)\n",
    "[REF 2](https://zhuanlan.zhihu.com/p/110769361)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的DQN\n",
    "\n",
    "只需要用DNN代替前面的Q table即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net: QTable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "    \n",
    "dqn = Net(n_feature=1, n_hidden=20, n_output=4)\n",
    "\n",
    "\n",
    "def trainDQN(x, y):\n",
    "    optimizer = torch.optim.SGD(dqn.parameters(), lr=0.02)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    for t in range(100):\n",
    "        py = dqn(x)\n",
    "        loss = loss_func(py, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: -148.84646666666666\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "rewards = []\n",
    "\n",
    "for episode in range(50):\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = 0\n",
    "\n",
    "    for step in range(max_step):\n",
    "        rnd = random.uniform(0,1)\n",
    "        # \n",
    "        x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "    \n",
    "        qs = dqn(x).data.numpy()[0]\n",
    "        \n",
    "        \n",
    "        if rnd > epsilon:\n",
    "            action = np.argmax(qs)\n",
    "        else:\n",
    "            action = random.sample(avail_actions(state),1)[0]\n",
    "            \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        \n",
    "        ## Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qs[action] = qs[action] + learning_rate*(reward+gamma*np.max(qs) - qs[action])\n",
    "        \n",
    "        \n",
    "        # update DQN\n",
    "        x = torch.from_numpy(np.array([[state]])).type(torch.FloatTensor)\n",
    "        y = torch.from_numpy(np.array([qs])).type(torch.FloatTensor)\n",
    "        trainDQN(x, y)    \n",
    "    \n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "print(\"Score over time: \" + str(sum(rewards)/total_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Solve the Maze\n",
    "\n",
    "for episode in range(1):\n",
    "    state = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "    \n",
    "    for step in range(max_step):\n",
    "        qs = dqn(x).data.numpy()[0]\n",
    "        action = np.argmax(qs)\n",
    "        \n",
    "        new_state, reward, done = go_one_step(state, action)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        actions.append(action)\n",
    "        \n",
    "    print(list(map(lambda a: ACTIONS_STR[a], actions)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 三个优化\n",
    "\n",
    "![](dqn01.png)\n",
    "![](dqn02.png)\n",
    "![](dqn03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
