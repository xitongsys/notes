{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy [REF](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "![](ce01.png)\n",
    "![](ce02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 最大化似然函数就等价于最小化交叉熵\n",
    "\n",
    "2. $\\sum_i^n p_i log(q_i)$ 这个函数的最大值就是当$q_i = p_i$的时候，考虑拉格朗日乘子法\n",
    "\n",
    "$g = \\sum_{i=0}^n p_i log(q_i) - \\lambda (\\sum_{i=0}^n q_i - 1)$\n",
    "\n",
    "所以有 $\\frac{\\partial g}{\\partial q_i} = \\frac{p_i}{q_i} - \\lambda = 0$\n",
    "\n",
    "即 $\\frac{p_i}{q_i} = \\lambda $。所有的都一样，所以$q_i = p_i$\n",
    "\n",
    "3. 最小化cross entropy的过程，就是让$q_i$ 不断趋近于 $p_i$。所以可以用cross entropy来作为分类问题的loss function"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
